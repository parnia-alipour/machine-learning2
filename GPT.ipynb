{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T18:10:55.208339Z",
     "start_time": "2025-11-13T18:10:55.205852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers.integrations.higgs import pad_to_block"
   ],
   "id": "14a656fbf55a411b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T18:10:58.327781Z",
     "start_time": "2025-11-13T18:10:55.699190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name=\"THUDM/chatglm-6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True\n",
    ")\n"
   ],
   "id": "8954d7e5faf23189",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ChatGLMTokenizer has no attribute vocab_size",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m model_name=\u001B[33m\"\u001B[39m\u001B[33mTHUDM/chatglm-6b\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m tokenizer = \u001B[43mAutoTokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m      3\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1125\u001B[39m, in \u001B[36mAutoTokenizer.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[39m\n\u001B[32m   1123\u001B[39m     _ = kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mcode_revision\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m   1124\u001B[39m     tokenizer_class.register_for_auto_class()\n\u001B[32m-> \u001B[39m\u001B[32m1125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtokenizer_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1126\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m   1127\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1128\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m config_tokenizer_class \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1129\u001B[39m     tokenizer_class = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2097\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001B[39m\n\u001B[32m   2094\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2095\u001B[39m         logger.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mloading file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m from cache at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresolved_vocab_files[file_id]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m2097\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_from_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2098\u001B[39m \u001B[43m    \u001B[49m\u001B[43mresolved_vocab_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2099\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2100\u001B[39m \u001B[43m    \u001B[49m\u001B[43minit_configuration\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2101\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2102\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2103\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2104\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2105\u001B[39m \u001B[43m    \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2106\u001B[39m \u001B[43m    \u001B[49m\u001B[43m_is_local\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_local\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2107\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2108\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2109\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2343\u001B[39m, in \u001B[36mPreTrainedTokenizerBase._from_pretrained\u001B[39m\u001B[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001B[39m\n\u001B[32m   2341\u001B[39m \u001B[38;5;66;03m# Instantiate the tokenizer.\u001B[39;00m\n\u001B[32m   2342\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2343\u001B[39m     tokenizer = \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43minit_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minit_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2344\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m import_protobuf_decode_error():\n\u001B[32m   2345\u001B[39m     logger.info(\n\u001B[32m   2346\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2347\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   2348\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm_hyphen_6b\\bf0f5cfb575eebebf9b655c5861177acfee03f16\\tokenization_chatglm.py:196\u001B[39m, in \u001B[36mChatGLMTokenizer.__init__\u001B[39m\u001B[34m(self, vocab_file, do_lower_case, remove_space, bos_token, eos_token, end_token, mask_token, gmask_token, padding_side, pad_token, unk_token, num_image_tokens, **kwargs)\u001B[39m\n\u001B[32m    180\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\n\u001B[32m    181\u001B[39m         \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    182\u001B[39m         vocab_file,\n\u001B[32m   (...)\u001B[39m\u001B[32m    194\u001B[39m         **kwargs\n\u001B[32m    195\u001B[39m ) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m196\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    197\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdo_lower_case\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdo_lower_case\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    198\u001B[39m \u001B[43m        \u001B[49m\u001B[43mremove_space\u001B[49m\u001B[43m=\u001B[49m\u001B[43mremove_space\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    199\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    200\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbos_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbos_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    201\u001B[39m \u001B[43m        \u001B[49m\u001B[43meos_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43meos_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    202\u001B[39m \u001B[43m        \u001B[49m\u001B[43mend_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43mend_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    203\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmask_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    204\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgmask_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgmask_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    205\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpad_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpad_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    206\u001B[39m \u001B[43m        \u001B[49m\u001B[43munk_token\u001B[49m\u001B[43m=\u001B[49m\u001B[43munk_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    207\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_image_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_image_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    208\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    209\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    211\u001B[39m     \u001B[38;5;28mself\u001B[39m.do_lower_case = do_lower_case\n\u001B[32m    212\u001B[39m     \u001B[38;5;28mself\u001B[39m.remove_space = remove_space\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\tokenization_utils.py:438\u001B[39m, in \u001B[36mPreTrainedTokenizer.__init__\u001B[39m\u001B[34m(self, **kwargs)\u001B[39m\n\u001B[32m    434\u001B[39m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__init__\u001B[39m(**kwargs)\n\u001B[32m    436\u001B[39m \u001B[38;5;66;03m# 4. If some of the special tokens are not part of the vocab, we add them, at the end.\u001B[39;00m\n\u001B[32m    437\u001B[39m \u001B[38;5;66;03m# the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES following `tokenizers`\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m438\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_add_tokens\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    439\u001B[39m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mall_special_tokens_extended\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_added_tokens_encoder\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    440\u001B[39m \u001B[43m    \u001B[49m\u001B[43mspecial_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    441\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    443\u001B[39m \u001B[38;5;28mself\u001B[39m._decode_use_source_tokenizer = \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\tokenization_utils.py:546\u001B[39m, in \u001B[36mPreTrainedTokenizer._add_tokens\u001B[39m\u001B[34m(self, new_tokens, special_tokens)\u001B[39m\n\u001B[32m    544\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m added_tokens\n\u001B[32m    545\u001B[39m \u001B[38;5;66;03m# TODO this is fairly slow to improve!\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m546\u001B[39m current_vocab = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m.copy()\n\u001B[32m    547\u001B[39m new_idx = \u001B[38;5;28mlen\u001B[39m(current_vocab)  \u001B[38;5;66;03m# only call this once, len gives the last index + 1\u001B[39;00m\n\u001B[32m    548\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m new_tokens:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm_hyphen_6b\\bf0f5cfb575eebebf9b655c5861177acfee03f16\\tokenization_chatglm.py:248\u001B[39m, in \u001B[36mChatGLMTokenizer.get_vocab\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    246\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_vocab\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    247\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\" Returns vocab as a dict \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m248\u001B[39m     vocab = {\u001B[38;5;28mself\u001B[39m._convert_id_to_token(i): i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mvocab_size\u001B[49m)}\n\u001B[32m    249\u001B[39m     vocab.update(\u001B[38;5;28mself\u001B[39m.added_tokens_encoder)\n\u001B[32m    250\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m vocab\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1127\u001B[39m, in \u001B[36mSpecialTokensMixin.__getattr__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   1124\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.convert_tokens_to_ids(attr_as_tokens) \u001B[38;5;28;01mif\u001B[39;00m attr_as_tokens \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1126\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.\u001B[34m__dict__\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1127\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m has no attribute \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m   1128\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1129\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__getattr__\u001B[39m(key)\n",
      "\u001B[31mAttributeError\u001B[39m: ChatGLMTokenizer has no attribute vocab_size"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T17:51:26.196863Z",
     "start_time": "2025-11-13T17:51:26.184253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chat(user):\n",
    "    ids=tokenizer.encode(user,return_tensors=\"pt\")\n",
    "    att=torch.ones_like(ids)\n",
    "\n",
    "    oput=model.generate(ids,max_length=20\n",
    "                        ,pad_token_id=tokenizer.eos_token_id,\n",
    "                        attention_mask=att,\n",
    "                        temperature=0.9,\n",
    "                        repetition_penalty=1.2,\n",
    "                        top_p=0.9,\n",
    "                        do_sample=True,)\n",
    "    re=tokenizer.decode(oput[0],skip_special_tokens=True)\n",
    "    return re"
   ],
   "id": "b0328b297c01de1d",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T17:54:52.539141Z",
     "start_time": "2025-11-13T17:51:26.769908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "while True:\n",
    "    text=input(\"enter your message: \")\n",
    "    if text.lower() in [\"خروج\"]:\n",
    "        print('خداحافظ')\n",
    "        break\n",
    "    print(chat(text))"
   ],
   "id": "d5516136630c935d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام حالت چطوره؟ نه! یعنی اون فرد رو انتخاب کردم و این شخص چه کسی هست که می\n",
      "سلام میتونی بگی بهم حافظه رم چیه؟ --منم یه کاری کردم که بتونم این کارو بکنم--\n",
      "چکار کردی و آن دو مرد را به خانه برد. در آنجا، دختر خود و همسرش به\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m----> 2\u001B[0m     text\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43menter your message: \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m text\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mخروج\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m      4\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mخداحافظ\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mD:\\ml2\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py:1396\u001B[0m, in \u001B[0;36mKernel.raw_input\u001B[1;34m(self, prompt)\u001B[0m\n\u001B[0;32m   1394\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1395\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[1;32m-> 1396\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1397\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1398\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_shell_context_var\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_shell_parent_ident\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1399\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1400\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1401\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\ml2\\.venv\\lib\\site-packages\\ipykernel\\kernelbase.py:1441\u001B[0m, in \u001B[0;36mKernel._input_request\u001B[1;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[0;32m   1438\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[0;32m   1439\u001B[0m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[0;32m   1440\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInterrupted by user\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1442\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1443\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Message:\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d10d78671b123e93"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
